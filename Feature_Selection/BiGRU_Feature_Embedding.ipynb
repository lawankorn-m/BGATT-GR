{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# à¸ºBiGRU Embedding","metadata":{}},{"cell_type":"code","source":"#BiGRU Embedding\nimport pandas as pd\nimport numpy as np\nfrom keras.models import Model\nfrom keras.layers import GRU, Bidirectional, LSTM, Dense, Dropout, Input, BatchNormalization, Attention, Layer\nfrom keras.optimizers import Adam\nfrom sklearn.preprocessing import StandardScaler\nimport tensorflow as tf\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\n\n# Enable TensorFlow's eager execution\ntf.config.experimental_run_functions_eagerly(True)\n\nfile_paths = [\n    \"Data/Fused_Feature_Train.csv\",\n]\n\n# Read and preprocess CSV files\ndf = pd.read_csv(file_paths[0])\ndf = df.iloc[:, 1:-1]  # Remove the last column\n\n# Custom Mean Reduction Layer\nclass MeanReduction(Layer):\n    def call(self, inputs):\n        return tf.reduce_mean(inputs, axis=1)\n\n# Custom Reshape Layer\nclass ReshapeLayer(Layer):\n    def __init__(self, new_shape):\n        super(ReshapeLayer, self).__init__()\n        self.new_shape = new_shape\n\n    def call(self, inputs):\n        return tf.reshape(inputs, self.new_shape)\n\n# Function to create and predict models\ndef create_and_predict(model_type, Dim_value, X):\n    input_shape = (X.shape[1], X.shape[2])\n    inputs = Input(shape=input_shape)\n    x = Bidirectional(GRU(units=int(Dim_value / 2), return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.01)))(inputs)\n   \n    # Attention layer\n    attention = Attention()([x, x])\n    attention = MeanReduction()(attention)  # Custom layer to reduce mean\n    \n    # Ensure fully-defined shape before BatchNormalization\n    new_shape = [-1, attention.shape[-1]]  # Shape to reshape attention output\n    attention = ReshapeLayer(new_shape=new_shape)(attention)  # Using custom reshape layer\n    \n    x = BatchNormalization()(attention)\n    x = Dropout(0.3)(x)  # Adjusted dropout rate\n    outputs = Dense(X.shape[2], activation='linear')(x)\n    \n    model = Model(inputs, outputs)\n    \n    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=1e-4))  # Smaller learning rate\n    \n    # Early stopping and learning rate reduction callbacks\n    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n    \n    # Train the model with early stopping\n    model.fit(X, X, epochs=1000, batch_size=16, verbose=1, validation_split=0.2, callbacks=[early_stopping, reduce_lr])\n    \n    # Extract features using the encoder part of the autoencoder\n    encoder_model = Model(inputs, attention)  # Create a model from inputs to the attention layer\n    features = encoder_model.predict(X)\n    \n    # Save features to CSV\n    data_csv = pd.DataFrame(data=features)\n    new_column_names = [f'{model_type}-{Dim_value}_{i}' for i in range(features.shape[1])]\n    data_csv.columns = new_column_names\n    data_csv.to_csv(f'{model_type}-{Dim_value}_train.csv', index=True)\n\n# Preprocess data\ndata = df.apply(pd.to_numeric, errors='coerce')\ndata.dropna(inplace=True)\ndata_scaled = StandardScaler().fit_transform(data.values)\nX = np.reshape(data_scaled, (data_scaled.shape[0], 1, data_scaled.shape[1]))\n\n# Call the models \ncreate_and_predict('BiGRU', 4000, X)\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}